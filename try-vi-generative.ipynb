{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build thêm data cho model generation từ pretrained model NlpHUST/gpt2-vietnamese\n",
    "(Inprogress)\n",
    "\n",
    "- Ở đây mình đang thử cách dùng adapter, để chỉ có thể build thêm data cho model trên máy tính cá nhân (chứ mấy cái server khủng lấy đâu ra :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1248: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NlpHUST/gpt2-vietnamese\")\n",
    "model = AutoModelWithLMHead.from_pretrained('NlpHUST/gpt2-vietnamese')\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('<|endoftext|>', 50257)])\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.get_vocab().items())\n",
    "items = tokenizer.get_added_vocab().items()\n",
    "print(items)\n",
    "char = 'lượng'\n",
    "for k, v in items:\n",
    "    if char in k:\n",
    "        print(k,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Generated text 1\n",
      "\n",
      "\n",
      "Loạt idol đọ dáng\n",
      "Mới đây, trên trang cá nhân của mình, Seungri đã đăng tải những khoảnh khắc cực đáng yêu của các thành viên trong nhóm Big Bang.\n",
      "Trong loạt ảnh này, các fan đã không thể rời mắt khỏi thần tượng bởi vẻ đẹp không góc chết của họ. Đặc biệt là khi Seungri xuất hiện trong bộ trang phục màu đỏ quyến rũ, khoe được đôi chân dài miên man của anh chàng. Đây cũng là lần đầu tiên Seungri gây chú ý với khán giả đến như vậy. Tuy nhiên, điều này không có nghĩa là Seungri sẽ rời bỏ nhóm trong tương lai gần. Hiện tại, anh vẫn chưa tiết lộ bất cứ thông tin nào về việc rời khỏi nhóm.(VietQ.vn) - Theo thông\n",
      "\n",
      "---\n",
      ">> Generated text 2\n",
      "\n",
      "\n",
      "Loạt idol đọ dáng\n",
      "Sau những ồn ào về mối quan hệ tình cảm với người hâm mộ, mới đây, các nghệ sĩ Hàn Quốc đã chính thức lên tiếng xác nhận tin đồn hẹn hò của mình.\n",
      "Trong một cuộc phỏng vấn với tạp chí Sports Illustrated hồi đầu tháng 9, Kim Tae Hee đã tiết lộ rằng cô đã tìm được một người đàn ông phù hợp để kết hôn vào cuối năm nay. Tuy nhiên, khi được hỏi về chuyện này, nữ diễn viên đã từ chối trả lời và cho biết: “Tôi không có ý định nói chuyện với bất kỳ ai cả. Tôi chỉ muốn nói với mọi người rằng tôi cảm thấy rất hạnh phúc với cuộc sống hiện tại”.\n",
      "Trước đó, trong một bài phỏng\n",
      "\n",
      "---\n",
      ">> Generated text 3\n",
      "\n",
      "\n",
      "Loạt idol đọ dáng\n",
      "(Kiến Thức) - Những hình ảnh mới nhất của các mỹ nhân Hàn Quốc khiến người hâm mộ không thể rời mắt.\n",
      "Những bức ảnh được chia sẻ trên mạng xã hội đã thu hút sự chú ý của đông đảo cư dân mạng, đặc biệt là các fan của nữ diễn viên Park Min Young, Park Shin Hye...\n",
      "Mới đây, trên trang cá nhân của mình, Kim Tae Hee bất ngờ đăng tải những dòng trạng thái đầy ẩn ý trên Instagram. Trong đó, cô viết: \"Hôm nay là ngày sinh nhật của tôi. Tôi muốn cảm ơn tất cả mọi người vì đã giúp đỡ tôi trong suốt thời gian vừa qua\".\n",
      "Được biết, đây không phải là lần đầu tiên nữ thần tượng\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "text = \"\"\"\n",
    "Loạt idol đọ dáng\n",
    "\"\"\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "max_length = 150\n",
    "\n",
    "sample_outputs = model.generate(input_ids,pad_token_id=tokenizer.eos_token_id,\n",
    "                                   do_sample=True,\n",
    "                                   max_length=max_length,\n",
    "                                   min_length=max_length,\n",
    "                                   top_k=40,\n",
    "                                   num_beams=5,\n",
    "                                   early_stopping=True,\n",
    "                                   no_repeat_ngram_size=2,\n",
    "                                   num_return_sequences=3)\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\">> Generated text {}\\n\\n{}\".format(i+1, tokenizer.decode(sample_output.tolist())))\n",
    "    print('\\n---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional_data [{'input_ids': [13621, 4692, 2339, 14483, 29840, 4850, 18, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [13621, 4692, 2339, 14483, 29840, 4850, 18, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258]}, {'input_ids': [44, 4646, 5270, 33180, 29840, 4850, 18, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [44, 4646, 5270, 33180, 29840, 4850, 18, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258]}, {'input_ids': [15514, 1638, 38895, 29840, 4850, 3192, 12265, 30965, 24307, 18, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [15514, 1638, 38895, 29840, 4850, 3192, 12265, 30965, 24307, 18, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258]}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# List of text data\n",
    "text_data = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"Here's another sentence.\",\n",
    "    \"And a third sentence for additional training.\"\n",
    "]\n",
    "\n",
    "# Encode text data into input IDs and labels\n",
    "additional_data = []\n",
    "for text in text_data:\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    example = {\n",
    "        'input_ids': encoded_text['input_ids'].squeeze().tolist(),\n",
    "        'attention_mask': encoded_text['attention_mask'].squeeze().tolist(),\n",
    "        'labels': encoded_text['input_ids'].squeeze().tolist()\n",
    "    }\n",
    "    additional_data.append(example)\n",
    "print('additional_data', additional_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sử dùng function fine tune từ blog sau\n",
    "https://212digital.medium.com/fine-tuning-the-gpt-2-large-language-model-unlocking-its-full-potential-66e3a082ab9c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.26.1\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def fine_tune_gpt2(model_name, train_file, output_dir):\n",
    "    # Load GPT-2 model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load training dataset\n",
    "    train_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=train_file,\n",
    "        block_size=128)\n",
    "    # Create data collator for language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False)\n",
    "    # Set training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=4,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "    # Train the model\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "    trainer.train()\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module transformers has no attribute PTuningOptimizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m LineByLineTextDataset, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n\u001b[1;32m      3\u001b[0m \u001b[39m# Load the training data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# train_data = TextDataset(\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m#     \"train.txt\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[39m# Initialize the P-tuning optimizer\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m optimizer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mPTuningOptimizer(model)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m model\u001b[39m.\u001b[39mtrain_adapter_from_pretrained(\n\u001b[1;32m     15\u001b[0m     additional_data,\n\u001b[1;32m     16\u001b[0m     num_train_steps\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m,\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[1;32m     18\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrained_model\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/utils/import_utils.py:1103\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m   1102\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objects:\n\u001b[0;32m-> 1103\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objects[name]\n\u001b[1;32m   1104\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules:\n\u001b[1;32m   1105\u001b[0m         value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module transformers has no attribute PTuningOptimizer"
     ]
    }
   ],
   "source": [
    "from transformers import LineByLineTextDataset, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Load the training data\n",
    "# train_data = TextDataset(\n",
    "#     \"train.txt\",\n",
    "#     tokenizer=tokenizer,\n",
    "#     block_size=512,\n",
    "# )\n",
    "\n",
    "# Initialize the P-tuning optimizer\n",
    "optimizer = transformers.PTuningOptimizer(model)\n",
    "\n",
    "# Train the model\n",
    "model.train_adapter_from_pretrained(\n",
    "    additional_data,\n",
    "    num_train_steps=10000,\n",
    "    optimizer=optimizer,\n",
    "    output_dir=\"trained_model\",\n",
    ")\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer, mlm=False, mlm_probability=0.15\n",
    "# )\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results-train-viblo\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=12,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     warmup_steps=1000,\n",
    "#     logging_steps=500,\n",
    "# )\n",
    "\n",
    "# # model.add_adapter(\"test-adapter\")\n",
    "# modelAdapter.train_adapter(\"test-adapter\", additional_data)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=additional_data,\n",
    "#     data_collator=data_collator\n",
    "# )\n",
    "\n",
    "# trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
